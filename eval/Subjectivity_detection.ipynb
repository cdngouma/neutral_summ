{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec8c50a-1809-4f67-afe4-d365aef17bad",
   "metadata": {},
   "source": [
    "## Evaluation de la présence de terms de subjectivité et leur puissance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4deba0-0d0e-411a-b0a8-6a7e0d8ed4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79380241-3ac1-4987-8f4c-de986b516947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [str(word) for word in nlp(str(text))]\n",
    "\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babfc18e-e15f-44c7-9c18-be8052237587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj_eval(file_path):\n",
    "    \n",
    "    subj_referenecs = pd.read_csv(\"./subjectivity_clues.csv\")\n",
    "    df_to_eval = pd.read_csv(file_path)\n",
    "    summaries = list(df_to_eval[\"summary\"])\n",
    "    #summaries = list(df_to_eval[\"summ_3\"])\n",
    "    \n",
    "    n_weak_scores, weak_scores = [], []\n",
    "    n_strong_scores,strong_scores = [], []\n",
    "    \n",
    "    for i, summary in enumerate(summaries):\n",
    "        n_weak_score, weak_score = 0., 0.\n",
    "        n_strong_score, strong_score = 0., 0.\n",
    "        token_list = tokenize(summary)\n",
    "        for token in token_list:\n",
    "            df_temp = subj_referenecs[subj_referenecs[\"word\"]==token]\n",
    "            if len(df_temp) > 0:\n",
    "                type_ = most_frequent(list(df_temp[\"type\"]))\n",
    "                if type_ == \"weaksubj\":\n",
    "                    weak_score += 1\n",
    "                    n_weak_score += 1/len(token_list)\n",
    "                if type_ == \"strongsubj\":\n",
    "                    strong_score +=1\n",
    "                    n_strong_score += 1/len(token_list)\n",
    "        \n",
    "        weak_scores.append(weak_score)\n",
    "        n_weak_scores.append(n_weak_score)\n",
    "        strong_scores.append(strong_score)\n",
    "        n_strong_scores.append(n_strong_score)\n",
    "    \n",
    "    return weak_scores, n_weak_scores, strong_scores, n_strong_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81867858-c557-4466-b15a-8d29a7634942",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meansum = \"../baselines/meansum_results_new.csv\"\n",
    "path_gpt2 = \"../baselines/gpt2_summaries.csv\"\n",
    "path_reference = \"../baselines/reference_summaries.csv\"\n",
    "path_textrank = \"../baselines/textrank_summaries.csv\"\n",
    "our_path = \"../outputs/train_300epochs.baseHtilt_cosineHhatversHhat_HtiltmeanHtiltcontext_FULL.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e3b031-6704-4db2-9862-9b612e0d719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_scores, n_weak_scores, strong_scores, n_strong_scores = subj_eval(our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a007e010-7ebe-4bd5-8123-029cc1fc199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6473429951690823\n",
      "0.5072463768115942\n"
     ]
    }
   ],
   "source": [
    "print(sum(weak_scores)/len(weak_scores))\n",
    "print(sum(strong_scores)/len(strong_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fc65e5-6ad7-42fb-9b44-e5be71b8b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016966197265221812\n",
      "0.007764534603548347\n"
     ]
    }
   ],
   "source": [
    "print(sum(n_weak_scores)/len(n_weak_scores))\n",
    "print(sum(n_strong_scores)/len(n_strong_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d54cb4-1dde-4f16-add7-a317cd450aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de document ou il y a plus de weak que de strong : 91\n",
      "nombre de document ou il y a plus de strong que de weak : 82\n",
      "nombre de document ou same : 34\n"
     ]
    }
   ],
   "source": [
    "count_weak = 0\n",
    "count_strong = 0\n",
    "count_neutre = 0\n",
    "for i,_ in enumerate(weak_scores):\n",
    "    if weak_scores[i] > strong_scores[i]:\n",
    "        count_weak += 1\n",
    "    elif weak_scores[i] < strong_scores[i]:\n",
    "        count_strong += 1\n",
    "    else:\n",
    "        count_neutre += 1\n",
    "print(\"nombre de document ou il y a plus de weak que de strong :\", count_weak)\n",
    "print(\"nombre de document ou il y a plus de strong que de weak :\", count_strong)\n",
    "print(\"nombre de document ou same :\", count_neutre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3d1d34-b1ca-409c-af57-2447ec29dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de document ou il y a plus de weak que de strong : 91\n",
      "nombre de document ou il y a plus de strong que de weak : 82\n",
      "nombre de document ou same : 34\n"
     ]
    }
   ],
   "source": [
    "count_weak = 0\n",
    "count_strong = 0\n",
    "count_neutre = 0\n",
    "for i,_ in enumerate(n_weak_scores):\n",
    "    if n_weak_scores[i] > n_strong_scores[i]:\n",
    "        count_weak += 1\n",
    "    elif n_weak_scores[i] < n_strong_scores[i]:\n",
    "        count_strong += 1\n",
    "    else:\n",
    "        count_neutre += 1\n",
    "print(\"nombre de document ou il y a plus de weak que de strong :\", count_weak)\n",
    "print(\"nombre de document ou il y a plus de strong que de weak :\", count_strong)\n",
    "print(\"nombre de document ou same :\", count_neutre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad80563-471e-4116-b4dd-6e920098c2eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7354f3f9-23fe-449a-9bc6-bdbfb66dc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e170383-69ee-4ff8-9a1d-a7924a5d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vader(text, filter_POS=['PUNCT', 'DET'], stopwords_n=[]):\n",
    "    list_ = []\n",
    "    for token in nlp(str(text)):\n",
    "        if token.text in stopwords_n:\n",
    "            continue\n",
    "        elif token.pos_ in filter_POS:\n",
    "            continue\n",
    "        else:\n",
    "            list_.append(token.text.lower())\n",
    "    return ' '.join(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ec19cd-1374-4108-b8cf-16e67cc083b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(file_path):\n",
    "     \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "     \n",
    "    df = pd.read_csv(file_path)\n",
    "    summaries = list(df[\"summary\"])\n",
    "    #summaries = list(df[\"summ_2\"])\n",
    "    \n",
    "    neu_dim, pos_dim, neg_dim = 0, 0, 0\n",
    "    neg_comp, pos_comp, neu_comp= 0, 0, 0\n",
    "\n",
    "    for summary in summaries : \n",
    "        summary = tokenize_vader(summary, stopwords_n=stopwords_nltk)\n",
    "        sentiment_dict = sid_obj.polarity_scores(summary)\n",
    "        ########### 1 -- -Getting main dimension\n",
    "        neg_score = sentiment_dict['neg']\n",
    "        neu_score = sentiment_dict['neu']\n",
    "        pos_score = sentiment_dict['pos']\n",
    "\n",
    "        main_dim_sent = sentiment_dict.copy()\n",
    "        del main_dim_sent['compound']\n",
    "        dimension = (max(main_dim_sent, key=main_dim_sent.get))\n",
    "        if dimension == 'neg':\n",
    "            neg_dim += 1\n",
    "        if dimension == 'pos':\n",
    "            pos_dim += 1\n",
    "        if dimension == 'neu':\n",
    "            neu_dim += 1\n",
    "        ################## 2 -- Getting the compound sentiment of summary\n",
    "        if sentiment_dict['compound'] >= 0.6 :\n",
    "            pos_comp += 1\n",
    "        elif sentiment_dict['compound'] <= - 0.6 :\n",
    "            neg_comp += 1\n",
    "        else: \n",
    "            neu_comp += 1\n",
    "            \n",
    "    return neu_dim, pos_dim, neg_dim, neg_comp, pos_comp, neu_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7780f3-89a8-4bfa-9c29-382cc4e57104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 57, 0, 0, 96, 111)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores(our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bfa68-4807-4fc3-80a4-f533eaacfce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
