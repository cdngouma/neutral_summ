{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5608fe95-87a4-445a-b0f1-dbb6af3f4bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngoum\\anaconda3\\envs\\dl-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Personnalized libraries\n",
    "from configs.config import DatasetConfig, HP\n",
    "from data.DataLoader import build_dataloader\n",
    "from utils.Errors import loss_estimation\n",
    "from Procedures import Procedure\n",
    "from model.lm import LanguageModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ds_config = DatasetConfig()\n",
    "hp = HP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590504c-ddf7-46d1-8f44-f55ea62fa2b4",
   "metadata": {},
   "source": [
    "# Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d28cdb4-43d1-4a0d-a3c6-0642369846fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|█████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 207.36item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 (6.32%) duplicated reviews added.\n",
      "11 imbalanced batches found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Build vocabulary: 100%|███████████████████████████████████████████████████████████| 2272/2272 [00:22<00:00, 101.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instanciate dataloader for train, valid and test\n",
    "train_iter, vocab, _ = build_dataloader(\n",
    "    file_path=ds_config.train_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=None,\n",
    "    is_train=True,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "train_size = len(train_iter)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2508f8c-49ac-43a7-9dd3-e55b2352919f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 146.90item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 (5.87%) duplicated reviews added.\n",
      "4 imbalanced batches found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iter, _, valid_references = build_dataloader(\n",
    "    file_path=ds_config.valid_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=vocab,\n",
    "    is_train=False,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "valid_size = len(valid_iter)\n",
    "valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d44b76e-df89-4172-a8dd-b871d2347047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████████████████████████████████████████████████████████| 207/207 [00:00<00:00, 1280.73item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.00%) duplicated reviews added.\n",
      "0 imbalanced batches found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter, _, test_references = build_dataloader(\n",
    "    file_path=ds_config.test_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=vocab,\n",
    "    is_train=False,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=15,#ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "test_size = len(test_iter)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362083f8-b6c8-48c1-b085-d330dd22ad14",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "312cfa25-5579-4fae-b20f-d938e54003e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a497f2bc-09e6-42ca-b12d-c6cbc3f1f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Procedures import Procedure\n",
    "from model.lm import LanguageModel\n",
    "from configs.config import DatasetConfig, HP\n",
    "\n",
    "ds_config = DatasetConfig()\n",
    "hp = HP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c2e4de-4085-4378-9d13-9b6ffa0707b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbfc62c-8f65-4538-9b85-793d83bcd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = follow[\"Name\"]\n",
    "writer = SummaryWriter(comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd77c0d8-c299-4a0b-a328-971ce79dd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure = Procedure(vocab, writer=writer, train_ter=train_iter, valid_iter=valid_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992860f9-de29-43da-b696-6a35eb503170",
   "metadata": {},
   "source": [
    "### Train the Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5f6fed-7757-493b-8ce4-03f3498357f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:26, 2.23MB/s]                                                                    \n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:22<00:00, 17573.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Language Model for 300 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████                  | 218/284 [02:00<00:36,  1.81batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocedure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\coding\\neutral_summ\\Procedures.py:87\u001b[0m, in \u001b[0;36mProcedure.train_lm\u001b[1;34m(self, path, tolerance, check_every)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Update gamma for GRL\u001b[39;00m\n\u001b[0;32m     86\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp\u001b[38;5;241m.\u001b[39mgrl_gamma \u001b[38;5;241m*\u001b[39m (epoch\u001b[38;5;241m/\u001b[39mn_epochs))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m---> 87\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\Documents\\coding\\neutral_summ\\Procedures.py:220\u001b[0m, in \u001b[0;36mProcedure.run_epochs\u001b[1;34m(self, epoch, iterator_, cls_weight, rec_weight, alpha, model, mode, debug)\u001b[0m\n\u001b[0;32m    217\u001b[0m accumulation_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 220\u001b[0m     outputs, cls_preds, cls_preds_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_senti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# output = [seq_len, batch_size, output_dim]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Compute reconstruction loss\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\coding\\neutral_summ\\model\\lm.py:198\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, src_input, trg, src_senti, alpha, tf_ratio)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m######## Decoding\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp\u001b[38;5;241m.\u001b[39muse_rec:\n\u001b[1;32m--> 198\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\coding\\neutral_summ\\model\\lm.py:156\u001b[0m, in \u001b[0;36mLanguageModel.decode\u001b[1;34m(self, rec_hidden, trg, revs, tf_ratio)\u001b[0m\n\u001b[0;32m    154\u001b[0m prob \u001b[38;5;241m=\u001b[39m logits_to_prob(output, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m, gumbel_hard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m outputs[t] \u001b[38;5;241m=\u001b[39m prob\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m#decide if we are going to use teacher forcing or not\u001b[39;00m\n\u001b[0;32m    158\u001b[0m teacher_force \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m tf_ratio \u001b[38;5;28;01mif\u001b[39;00m tf_ratio \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "procedure.train_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "098cf239-441f-4f78-9556-29916cea8b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model from ./outputs/fullLM.batch_8_docs.180_epochs.Htilt.lr0.0001.pt\n"
     ]
    }
   ],
   "source": [
    "reviews = procedure.generate(itr=train_iter, lm_path=\"./outputs/fullLM.batch_8_docs.180_epochs.Htilt.lr0.0001.pt\", batch_idx=[1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acadbdd-77f9-4e09-a523-02f82c825e1c",
   "metadata": {},
   "source": [
    "#### Save the reconstructed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82998312-0fef-4e4b-a27f-c885aa2b76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(og, rec):\n",
    "    x = set(og.split())\n",
    "    y = set(rec.split())\n",
    "    z = x.intersection(y)\n",
    "    return round(100 * len(z) / len(x), 2)\n",
    "\n",
    "df = []\n",
    "\n",
    "for rev in reviews:\n",
    "    pid, rec_rev, og_rev = rev\n",
    "    df.append({\n",
    "        \"dataset\": \"train\",\n",
    "        \"prod_id\": pid,\n",
    "        \"original\": og_rev,\n",
    "        \"reconstructed\": rec_rev,\n",
    "        \"% match\": match(og_rev, rec_rev)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(\"./lm_180epochs.reconst_reviews_train.full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bd730-a1e5-4e9f-9d91-aeb12a444a49",
   "metadata": {},
   "source": [
    "### Train the summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daba61e7-f23a-4e7b-ae2e-debc09186c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model from ./outputs/baseLM..batch_8_docs.300_epochs.baseH.lr0.0001.pt\n",
      "Training Summarizer for 0 epochs...\n"
     ]
    }
   ],
   "source": [
    "procedure.train(lm_path=\"./outputs/baseLM..batch_8_docs.300_epochs.baseH.lr0.0001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b8f9c75-6140-4dd2-8124-9bacce490ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(list(range(0, 1460, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a604241-5a84-4131-99ff-5a1774d4af06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading summarizer from outputs/summ.fullLM.batch_8_docs.lm_lr0.0001.0_epochs.dec_baseH.sum_baseH.lr1e-05.v3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 284/284 [1:28:50<00:00, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed after 5330.724611 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summaries, hiddens, mean_hiddens = procedure.generate_summaries(itr=train_iter, batch_idx=None, path=\"outputs/summ.fullLM.batch_8_docs.lm_lr0.0001.0_epochs.dec_baseH.sum_baseH.lr1e-05.v3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde24-c907-4edc-b915-468e02fadb81",
   "metadata": {},
   "source": [
    "#### Save the generated summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4931267-be6a-4a10-958f-af682ce85b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for e in summaries:\n",
    "    prod_id = e[0]\n",
    "    summary = e[1][0]\n",
    "    df.append({\n",
    "        \"dataset\": \"train\",\n",
    "        #\"model\": \"full AE. Rec w/ H_tilt. Finetune w/ Hhat. Cos sim w/ H. Gen w/ Hhat\", \n",
    "        \"prod_id\": prod_id, \n",
    "        \"summary\": summary\n",
    "    })\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(\"./outputs/train_300epochs.summaries_v5_20221227__baseLM.no_finetuning.greedy_dec.gumbel.mini.v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab3ccc7e-08fd-4b00-ba7a-a405d56c5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"saved_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8edddfea-ff20-41ee-b0f2-e7547c2771cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"saved_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fcf4e-e5f9-49cb-b0b6-5905c41f4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bca3f-558b-466c-a2c5-5ad7e96ece29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
