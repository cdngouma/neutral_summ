{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5608fe95-87a4-445a-b0f1-dbb6af3f4bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngoum\\anaconda3\\envs\\dl-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Personnalized libraries\n",
    "from configs.config import DatasetConfig, HP\n",
    "from data.DataLoader import build_dataloader\n",
    "from utils.Errors import loss_estimation\n",
    "from Procedures import Procedure\n",
    "from model.lm import LanguageModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ds_config = DatasetConfig()\n",
    "hp = HP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ed85d-1505-44af-87d2-4bdaf2aee294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_data as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7c2dc-f845-4e15-86c2-967fa5241c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.preprocess(\"./data/processed/train.min.csv\")\n",
    "preprocess.preprocess(\"./data/processed/valid.min.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590504c-ddf7-46d1-8f44-f55ea62fa2b4",
   "metadata": {},
   "source": [
    "## Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d28cdb4-43d1-4a0d-a3c6-0642369846fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|█████████████████████████████████████████████████████████████████| 80/80 [00:00<00:00, 691.13item/s]\n",
      "Build vocabulary: 100%|███████████████████████████████████████████████████████████| 3218/3218 [00:31<00:00, 103.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instanciate dataloader for train, valid and test\n",
    "train_iter, vocab, _ = build_dataloader(\n",
    "    file_path=ds_config.train_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=None,\n",
    "    is_train=True,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "train_size = len(train_iter)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2508f8c-49ac-43a7-9dd3-e55b2352919f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 354.40item/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iter, _, valid_references = build_dataloader(\n",
    "    file_path=ds_config.valid_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=vocab,\n",
    "    is_train=False,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "valid_size = len(valid_iter)\n",
    "valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d44b76e-df89-4172-a8dd-b871d2347047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 947.22item/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter, _, test_references = build_dataloader(\n",
    "    file_path=ds_config.test_data, \n",
    "    vocab_size=ds_config.vocab_size,\n",
    "    vocab_min_freq=ds_config.min_freq,\n",
    "    vocab=vocab,\n",
    "    is_train=False,\n",
    "    shuffle_batch=False,\n",
    "    max_num_reviews=ds_config.max_num_reviews,\n",
    "    refs_path=None,\n",
    "    max_len_rev=ds_config.max_len_rev,\n",
    "    pin_memory=ds_config.pin_memory,\n",
    "    num_workers=ds_config.workers,\n",
    "    batch_size=ds_config.batch_size,\n",
    "    device=ds_config.device\n",
    ")\n",
    "\n",
    "test_size = len(test_iter)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362083f8-b6c8-48c1-b085-d330dd22ad14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcafcf1-d45c-48ea-acdd-97b0e3cff9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349cae0a-5302-42e9-ac1d-80d49a134220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import postprocess, is_special, clean_up_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5754be-e531-45c9-ac6d-a6410ae2177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        #src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93481e5b-172c-4438-b26d-d50524c08215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, context):\n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #context = [n layers * n directions, batch size, hid dim]\n",
    "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #context = [1, batch size, hid dim]\n",
    "        input_ = input_.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input_))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        emb_con = torch.cat((embedded, context), dim = 2)\n",
    "        #emb_con = [1, batch size, emb dim + hid dim]\n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
    "        #output = [batch size, emb dim + hid dim * 2]\n",
    "        prediction = self.fc_out(output)\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827023c1-ef5c-4ff9-a2c7-70f0a64b537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        #last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)\n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input_ = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input_, hidden, context)\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f970c97-502b-4fd8-ad64-974d7ef1618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, check):  \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.enc_input.permute(1, 0).contiguous().cuda()\n",
    "        trg = batch.enc_input.permute(1, 0).contiguous().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if check and i % 5 == 0 and i <= 25:\n",
    "            reviews = inference(output, vocab)\n",
    "            #print(f\"[batch:{i}]\", output.permute(1,2,0).argmax(1)[:4])\n",
    "            for j,r in enumerate(reviews):\n",
    "                print(f\"[batch:{i}, itr: {j}]\", r)\n",
    "                if j >= 5:\n",
    "                    break\n",
    "            print(\"-\"*100)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7dbd2d-02ec-4489-abad-a09ea4952121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.enc_input.permute(1, 0).contiguous().cuda()\n",
    "            trg = batch.enc_input.permute(1, 0).contiguous().cuda()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c15dc18-76e6-4173-b234-fde2a20d457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(final_dists, vocab):\n",
    "    final_dists = final_dists.permute(1, 2, 0) # [batch_size, ext_vocab, gen_len]\n",
    "    rev_hyps = final_dists.argmax(1)  # [batch_size, seq_len]\n",
    "    rev_hyp_words = [vocab.outputids2words(hyp) for i, hyp in enumerate(rev_hyps)]\n",
    "    reviews = [postprocess(words, skip_special_tokens=True, clean_up_tokenization_spaces=True) for words in rev_hyp_words]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0af8ee-b5e6-428f-a4fb-ed56a29e01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe8576a-eff7-46ef-85b6-9163d01f76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fae8bf48-4c31-4859-aad5-708fdb3fc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ca3f98-9998-4c6f-b60c-84fdf49c0741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 36s\n",
      "\tTrain Loss: 6.392 | Train PPL: 597.161\n",
      "\t Val. Loss: 6.610 |  Val. PPL: 742.371\n",
      "Epoch: 02 | Time: 1m 40s\n",
      "\tTrain Loss: 5.661 | Train PPL: 287.531\n",
      "\t Val. Loss: 6.697 |  Val. PPL: 810.029\n",
      "Epoch: 03 | Time: 1m 38s\n",
      "\tTrain Loss: 5.436 | Train PPL: 229.634\n",
      "\t Val. Loss: 6.872 |  Val. PPL: 965.131\n",
      "Epoch: 04 | Time: 1m 38s\n",
      "\tTrain Loss: 5.276 | Train PPL: 195.599\n",
      "\t Val. Loss: 7.000 |  Val. PPL: 1096.433\n",
      "Epoch: 05 | Time: 1m 39s\n",
      "\tTrain Loss: 5.142 | Train PPL: 171.009\n",
      "\t Val. Loss: 7.239 |  Val. PPL: 1392.864\n",
      "Epoch: 06 | Time: 1m 35s\n",
      "\tTrain Loss: 5.012 | Train PPL: 150.279\n",
      "\t Val. Loss: 7.261 |  Val. PPL: 1423.819\n",
      "Epoch: 07 | Time: 1m 35s\n",
      "\tTrain Loss: 4.871 | Train PPL: 130.459\n",
      "\t Val. Loss: 7.321 |  Val. PPL: 1511.658\n",
      "Epoch: 08 | Time: 1m 35s\n",
      "\tTrain Loss: 4.729 | Train PPL: 113.182\n",
      "\t Val. Loss: 7.224 |  Val. PPL: 1372.635\n",
      "Epoch: 09 | Time: 1m 36s\n",
      "\tTrain Loss: 4.581 | Train PPL:  97.633\n",
      "\t Val. Loss: 7.374 |  Val. PPL: 1593.334\n",
      "[batch:0, itr: 0] i bought this product it it it it it it it a it it. it it it it it it it it it it it it to it it it it it it it awsome for beginer it it it not it lightening. it to use it\n",
      "[batch:0, itr: 1] the stone is a chisels. the oil is. the out of. blade is not the.. the blade to not.......\n",
      "[batch:0, itr: 2] i was a i chisels and i needed to my chisels. chisels..\n",
      "[batch:0, itr: 3] i bought this stone, but i sharpen sharpening chisels., but,, a, but. is not to be, but would the,, of the, but i would one of the, than that but i,, a,.\n",
      "[batch:0, itr: 4] i is not i i i i i i to i to use.. i i am it. product.\n",
      "[batch:0, itr: 5] this is a,, but i,, a lot a,, but,, but,,,,,,,, a,, but, not a,,,,,,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:5, itr: 0] i was not was not of the it like it, the,, it does not intent like it, clamp thing i it not, but it the it, but the,, not,,, but it did not it, but it\n",
      "[batch:5, itr: 1] good good for but for wrists. well....\n",
      "[batch:5, itr: 2] it very well it for it it it it is it it.. the price. it would it it well.....\n",
      "[batch:5, itr: 3] it is a, but it is it it is it is made.\n",
      "[batch:5, itr: 4] this stone and. fitting it was a to use it to use with than a. a than it. the. a...\n",
      "[batch:5, itr: 5] good good for sharpen the for the ink. much.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:10, itr: 0] i not buy but the. it was not. it little bit. i did it was not buy it. not... not...\n",
      "[batch:10, itr: 1] this not you need in the, but they not stay ink you ink cute. will. ink.......\n",
      "[batch:10, itr: 2] great. for my gift for ring hills....\n",
      "[batch:10, itr: 3] i did not buy it but stay in my humidifier.. i.. i... i would not buy.................\n",
      "[batch:10, itr: 4] i not not not but they not not work stay in place. all. i buy the... not.................\n",
      "[batch:10, itr: 5] it is a cheap it is it is not it is to be it in the it it to it it is it be. it. it is like it it it. it it it is it is it is.. it\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:15, itr: 0] i did but i well, not the the did not stay in. but the not to magnet. i the were not the the not the the magnet. the not not the the magnet the magnet time. not the the..\n",
      "[batch:15, itr: 1] i is is a. it is it the magnet. it is thing not work the the the the is is the. the is not is the is not the i would not the the it the breast. the the the is is the. is not the\n",
      "[batch:15, itr: 2] it is, but, it it it is it!!! it! it!!\n",
      "[batch:15, itr: 3] the machine are diagrams. the. not stay in the. the. the the the the end and the. not the the. the am the the the the the end of the the. the the and the. the of the\n",
      "[batch:15, itr: 4] i not not stay out of they stay in the...... stay stay. stay the.. at...........\n",
      "[batch:15, itr: 5] i have product and. the magnetic sensational magnetic. the magnetic effects. the shimmer.. the receiving complements on the shimmer.....\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:20, itr: 0] i was a gift, it was to my it was gray.. it was wife was to. it was it was ideal and it was ideal and was. and it was ideal.\n",
      "[batch:20, itr: 1] this is i is is a is a it is clumpy is of it it is and. it the little boobs. it is is clumpy and it the end.. i it is a little pillow! is me little pillow! it is to fit honeywell\n",
      "[batch:20, itr: 2] i product not 38 i did not seem i did not was to work in the magnets. i not. i i would not not fit. i and i would not work return it.. not.....\n",
      "[batch:20, itr: 3] i what a gift, but it approve it. it it............\n",
      "[batch:20, itr: 4] i i was not ring. it was it it was was was. was it it. it to fit the it was it the bemis and it it was it it was clumpy and it was clumpy and it was and magnet of the was it it the it\n",
      "[batch:20, itr: 5] i not work with it was not it it bemis humidifier it the. of the. stoves. and it was was not ideal it was disappointed with the... and it was not ideal.. and it was.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:25, itr: 0] the maroonis to were not to the the the box, the box to the the box the the box time. the the the little the box, out of the the the of the box the of the the is the the and is\n",
      "[batch:25, itr: 1] i was not the. my humidifier.. i wish this was was was not...\n",
      "[batch:25, itr: 2] the product the package. shipped in the the box, the use out of the box and the dryer. the box and to use. the box of the a the dryer and the dryer : are a a bit of the up the\n",
      "[batch:25, itr: 3] this is very,,, it fit a small,. not use, and,\n",
      "[batch:25, itr: 4] i was was, it fine., and was coiled all and of my dryer. i was. been a. product to\n",
      "[batch:25, itr: 5] the was was the the was the out of the the was the the the was the off the brush and the was not was.. was the.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10 | Time: 1m 47s\n",
      "\tTrain Loss: 4.432 | Train PPL:  84.109\n",
      "\t Val. Loss: 7.389 |  Val. PPL: 1618.726\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = vocab.pad()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "check = False\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP, check)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if epoch+2 == 10:\n",
    "        check = True\n",
    "    else:\n",
    "        check = False\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5b4790-a628-4097-a91f-090ce386da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 35s\n",
      "\tTrain Loss: 6.337 | Train PPL: 565.305\n",
      "\t Val. Loss: 6.656 |  Val. PPL: 777.160\n",
      "Epoch: 02 | Time: 1m 38s\n",
      "\tTrain Loss: 5.652 | Train PPL: 284.807\n",
      "\t Val. Loss: 6.696 |  Val. PPL: 809.394\n",
      "Epoch: 03 | Time: 1m 39s\n",
      "\tTrain Loss: 5.422 | Train PPL: 226.270\n",
      "\t Val. Loss: 6.846 |  Val. PPL: 940.292\n",
      "Epoch: 04 | Time: 1m 39s\n",
      "\tTrain Loss: 5.267 | Train PPL: 193.858\n",
      "\t Val. Loss: 7.005 |  Val. PPL: 1102.024\n",
      "Epoch: 05 | Time: 1m 39s\n",
      "\tTrain Loss: 5.122 | Train PPL: 167.654\n",
      "\t Val. Loss: 7.167 |  Val. PPL: 1295.786\n",
      "Epoch: 06 | Time: 1m 39s\n",
      "\tTrain Loss: 4.991 | Train PPL: 147.067\n",
      "\t Val. Loss: 7.214 |  Val. PPL: 1357.788\n",
      "Epoch: 07 | Time: 1m 39s\n",
      "\tTrain Loss: 4.835 | Train PPL: 125.872\n",
      "\t Val. Loss: 7.323 |  Val. PPL: 1514.604\n",
      "Epoch: 08 | Time: 1m 39s\n",
      "\tTrain Loss: 4.685 | Train PPL: 108.336\n",
      "\t Val. Loss: 7.262 |  Val. PPL: 1425.588\n",
      "Epoch: 09 | Time: 1m 39s\n",
      "\tTrain Loss: 4.511 | Train PPL:  91.017\n",
      "\t Val. Loss: 7.269 |  Val. PPL: 1435.282\n",
      "[batch:0, itr: 0] i bought this husband and it it it it it it to it it it it it it for it was it like it i it it it it it it was was it it and it my and it it not it it and it it it\n",
      "[batch:0, itr: 1] the stone is with the the inaccurate is the the the out of the middle of the the the of the the. out...\n",
      "[batch:0, itr: 2] i use. better than i. on my chisels. my chisels chisels.\n",
      "[batch:0, itr: 3] i have a hawk is a a of chisels and a a a a, and chisels a a i would to to sharpen the would the blade to a chisel kits, but i more expensive to a chisel and but i is a sharpen i\n",
      "[batch:0, itr: 4] i is is this kit for a i recommend this i a chisels and. i would not money. it\n",
      "[batch:0, itr: 5] this is a good job for a a i a a a than a i have a a of i would a a a a. a a a niceon a chisel who wants to product.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:5, itr: 0] i was this was disappointment. i was it nice. my face, it was not intent at, i was was a a. i was a to the to the the the. a to.. i i was a the it was the, i was\n",
      "[batch:5, itr: 1] i good for expected no problems...\n",
      "[batch:5, itr: 2] i it was it i was for a i i was it it for my face. it would it..\n",
      "[batch:5, itr: 3] i is is for i it it is not, it is not made. i it it it. is. it is..\n",
      "[batch:5, itr: 4] this. my, fitting the i was no not to and the with my face. a tool are very disappointed......\n",
      "[batch:5, itr: 5] works good for not a little retired contractor. smooth...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:10, itr: 0] i was a gift for. i was not. i cute.. i would not product a a. and and.\n",
      "[batch:10, itr: 1] works great! you have ring, you have to use to get able good for..\n",
      "[batch:10, itr: 2] works well. but not. the bracelet....\n",
      "[batch:10, itr: 3] i love it i expected it use it it...\n",
      "[batch:10, itr: 4] i not waste of money and i not. the vacuum the. all. i. the...............\n",
      "[batch:10, itr: 5] it is is a it it it is not it it to use it in it. it to it it it use work it it. it is it not buy it it it it is of it..\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:15, itr: 0] i sleek were not well, the the the the the the the the the the the the the the the the were the the the the the the the were not the the the the the the the the. the the\n",
      "[batch:15, itr: 1] i is is not. i is it of colors. it it is not the. the. the is is not. the the the is the. the. the the is not the it the.. the the the the the the is the is the\n",
      "[batch:15, itr: 2] it like and it to it it it stay it it and it. it. it will it it great and\n",
      "[batch:15, itr: 3] i thing but i the the. not stay to the the the primary thing. the the the the the is not the the the the the stay the than the the the the the the the. the was not the the the the the the the the me\n",
      "[batch:15, itr: 4] i not not the out of they the the to to stay. the the are not. stay in stay them\n",
      "[batch:15, itr: 5] the the product product was not the the the the and the with the.. on\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:20, itr: 0] it is a, but it was to be it, it, and it was it and it, and it it.\n",
      "[batch:20, itr: 1] this is a a a a gift. i is a a of a. i a it is a a large and it is is not a with a a. a a bit of a a. be a a bit of a large it of..\n",
      "[batch:20, itr: 2] i not a 38 ddd. i. i was not to to be it to the but i not to to i to be to sports bra.. i am shocked to be able comfy to...\n",
      "[batch:20, itr: 3] it is not fit. it is not it it...\n",
      "[batch:20, itr: 4] i am not the the polish.. it vacuum attachment is is not the the the. the to the the. i the the the of the the is it is the the the. i i is not. on the\n",
      "[batch:20, itr: 5] this is a bear bear. the with a towel and the the. of the.. the and the the was not a the the. it the cooled.. the... the a towel.....\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:25, itr: 0] the titty to be it to. the, the vacuum the vacuum attachment was folded up to vacuum.. the to be able to on the dryer. the to on the blockage of well\n",
      "[batch:25, itr: 1] this is not fit than... it product. a...\n",
      "[batch:25, itr: 2] the previous the a, the to bemis bemis, the a bemis humidifyer and a a bit of a lint to a a bit of. the lint out lint out of to out of a bit and up a a\n",
      "[batch:25, itr: 3] this is not, but, but not it, and is a...\n",
      "[batch:25, itr: 4] this is is adorable but. well,. not fit all.. it was not sucked out of lint.. sucked. the.\n",
      "[batch:25, itr: 5] the vacuum the the the brush the the the vacuum attachment was not the the the the the the the the the the the the the was not the the much. not work\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10 | Time: 1m 50s\n",
      "\tTrain Loss: 4.352 | Train PPL:  77.632\n",
      "\t Val. Loss: 7.277 |  Val. PPL: 1446.448\n"
     ]
    }
   ],
   "source": [
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder\n",
    "\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = vocab.pad()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "check = False\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP, check)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if epoch+2 == 10:\n",
    "        check = True\n",
    "    else:\n",
    "        check = False\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a59b98d-d749-4955-95ab-eee35d54768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 38s\n",
      "\tTrain Loss: 6.382 | Train PPL: 591.261\n",
      "\t Val. Loss: 6.577 |  Val. PPL: 718.542\n",
      "Epoch: 02 | Time: 1m 38s\n",
      "\tTrain Loss: 5.643 | Train PPL: 282.171\n",
      "\t Val. Loss: 6.720 |  Val. PPL: 828.858\n",
      "Epoch: 03 | Time: 1m 39s\n",
      "\tTrain Loss: 5.422 | Train PPL: 226.229\n",
      "\t Val. Loss: 6.888 |  Val. PPL: 980.236\n",
      "Epoch: 04 | Time: 1m 38s\n",
      "\tTrain Loss: 5.260 | Train PPL: 192.502\n",
      "\t Val. Loss: 6.982 |  Val. PPL: 1077.446\n",
      "Epoch: 05 | Time: 1m 39s\n",
      "\tTrain Loss: 5.114 | Train PPL: 166.350\n",
      "\t Val. Loss: 7.257 |  Val. PPL: 1418.249\n",
      "Epoch: 06 | Time: 1m 38s\n",
      "\tTrain Loss: 4.970 | Train PPL: 144.075\n",
      "\t Val. Loss: 7.187 |  Val. PPL: 1322.012\n",
      "Epoch: 07 | Time: 1m 38s\n",
      "\tTrain Loss: 4.832 | Train PPL: 125.406\n",
      "\t Val. Loss: 7.251 |  Val. PPL: 1409.351\n",
      "Epoch: 08 | Time: 1m 39s\n",
      "\tTrain Loss: 4.695 | Train PPL: 109.361\n",
      "\t Val. Loss: 7.349 |  Val. PPL: 1555.268\n",
      "Epoch: 09 | Time: 1m 38s\n",
      "\tTrain Loss: 4.552 | Train PPL:  94.833\n",
      "\t Val. Loss: 7.283 |  Val. PPL: 1455.646\n",
      "[batch:0, itr: 0] i purchased it it it it it and it i it it it the it and it fine. it it it it it i it it it it it it it it it fine. it it it it it not it it. it up\n",
      "[batch:0, itr: 1] the stone is a to to the the stone to the out of it is is it the it to are it. it out it. it. it. will it\n",
      "[batch:0, itr: 2] i did not my chisels i i and i. i. i the. and i..\n",
      "[batch:0, itr: 3] i bought this for a, but of the kits, a few, and a. the wheels are a to use,,, to of the of the, and, i stones.,,\n",
      "[batch:0, itr: 4] i is a this kit i i have used to use to use. i i am of the. i have to use....\n",
      "[batch:0, itr: 5] i is a,, kit, i, the,,,, i,,,,, are a,,,, a problems with,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:5, itr: 0] i was this product, for but it like well made of the. it is not a like it for a reason. it is a but, and it is a few, and very a,, but much, it use it a a, and it\n",
      "[batch:5, itr: 1] very good. i expected for. good\n",
      "[batch:5, itr: 2] i did. it was expected for the. it was very well. my price. it.\n",
      "[batch:5, itr: 3] it is a gift for it it it it, it is it made.\n",
      "[batch:5, itr: 4] this is a gift for the i have is have to use it with... than. the. ink. not.\n",
      "[batch:5, itr: 5] works great shall sharpen a for is a.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:10, itr: 0] i not waste for my. i not to use a as.. not i would not. than..\n",
      "[batch:10, itr: 1] if you you are a the, but are not a gradual turtle a good. you...\n",
      "[batch:10, itr: 2] this. for my daughter.. it....\n",
      "[batch:10, itr: 3] i did not it it it it in it...\n",
      "[batch:10, itr: 4] i did not big divot and i product not work in place. all...\n",
      "[batch:10, itr: 5] it is not well made for it it is it it to to it it it it it to use. it it it it. it it. like not it it. it it it..\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:15, itr: 0] i was are not late, but the i was not work,, but the the,, but did the were not the the the, of the, i was not, i did not job. was not.\n",
      "[batch:15, itr: 1] i product is not. i the it colors but and it does drawback is not. it is not the. not to the the the. the. the, it is not to the the the. i. i would not the...\n",
      "[batch:15, itr: 2] it like, fitting starter contractor, it used it magnetic colors and. it.. it. it is great..\n",
      "[batch:15, itr: 3] i honing are the the kit is not work as the the.. the the. the the the the was not the the to the the have the of the the the. the kit is is the. out. i the the the the the the\n",
      "[batch:15, itr: 4] i did not fitting out of the wear the magnetic to wear the magnets. they to the.. is like a sports bra like the....\n",
      "[batch:15, itr: 5] i the product day and i have been theirafter magnetic. i magnetic magnetic and i. i have to. on my.......\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:20, itr: 0] i is a gift and bemis a bemis fit, and humidifier,., and it is a, and prevented a lot!\n",
      "[batch:20, itr: 1] it! it is a bit a bit it is a a bit the and it is a it the dark and and is a is be a a bit groove on the dark and it guess it is be a a and. it is to sleep\n",
      "[batch:20, itr: 2] i is not 38 ddd. advertised. i was not to get get to return it and i not to and i not work to to bra to\n",
      "[batch:20, itr: 3] it what described. it is stay in it and it it\n",
      "[batch:20, itr: 4] i it was a and the but i i colors were was a and magnet was clumpy and clotted to sleep the a the the the polish of the.. i would not a the and and the was a the and the of i was a the\n",
      "[batch:20, itr: 5] this not waste of money. figure out this bemis humidifier the. the of the polish and i and it it was not to to the.. little bottles and wondered it was. all. the... it...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[batch:25, itr: 0] the previous reviewers are the blue. the, the is the the. the, it the it brush is the the brush is the brush the box of the the the the of the blockage of the the. the the able to brush and the well\n",
      "[batch:25, itr: 1] i love this my and my dryer and this product.\n",
      "[batch:25, itr: 2] the colors are the of the the of of and, this be out of. the. the the of the the the the the, the. of the the the. of. the of the were were not compressed and the in\n",
      "[batch:25, itr: 3] works great,,,,,, small,,,,, clean,\n",
      "[batch:25, itr: 4] this is not, this was not suck, supposed in the, of lint, and i was. it fit my dryer.. my...\n",
      "[batch:25, itr: 5] the colors are the the the the brush the vacuum the brush the not it was the off the. the it was the the brush was the the the.. not it the\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10 | Time: 1m 49s\n",
      "\tTrain Loss: 4.414 | Train PPL:  82.622\n",
      "\t Val. Loss: 7.312 |  Val. PPL: 1498.291\n"
     ]
    }
   ],
   "source": [
    "from model.lm import LanguageModel\n",
    "\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LanguageModel(vocab, hp=hp).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TRG_PAD_IDX = vocab.pad()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "check = False\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP, check)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if epoch+2 == 10:\n",
    "        check = True\n",
    "    else:\n",
    "        check = False\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8b8f6-c422-4187-893a-edfa91874e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
